{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NssbqqcIL_20",
        "outputId": "8aaf46a5-6476-49b5-98cf-d3ee99d2e2f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m816.1/816.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.8/132.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.0/509.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.6/105.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -qqq install  transformers\n",
        "!pip -qqq install langchain\n",
        "!pip -qqq install sentence-transformers\n",
        "!pip -qqq install faiss-gpu\n",
        "!pip -qqq install chromadb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import  WebBaseLoader\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA, LLMChain"
      ],
      "metadata": {
        "id": "ks13LOoiMign"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "HF_TOKEN= getpass(\"enter hf token: \")\n",
        "os.environ['HUGGINGFACE_ACCESS_TOKEN']=HF_TOKEN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wlsj3xfuMikJ",
        "outputId": "be489e2a-05ad-46d6-c80b-9115f821f3b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enter hf token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "E2dtfMTsNtel"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WEBSITE=\"https://blog.google/technology/ai/google-gemini-ai/#sundar-note\"\n",
        "\n",
        "data=WebBaseLoader(WEBSITE)\n",
        "data.request_per_secobd=1\n",
        "\n",
        "content=data.aload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dASuwocPN8wo",
        "outputId": "b16c91d7-d4c0-49f0-c185-78a0a624575a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching pages: 100%|##########| 1/1 [00:00<00:00,  4.76it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content"
      ],
      "metadata": {
        "id": "2SzFFtqxN80I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter=RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=20)\n",
        "\n",
        "chunked_doc=splitter.split_documents(content)"
      ],
      "metadata": {
        "id": "EjWAW4MzOvEv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunked_doc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g4h8O01OvMi",
        "outputId": "d182b768-2461-43c0-c064-3761fee8e0c9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Introducing Gemini: Google’s most capable AI model yet\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main content\\n\\n\\n\\n\\n\\n        The Keyword\\n      \\n\\n\\n\\n\\n    Introducing Gemini: our largest and most capable AI model', metadata={'source': 'https://blog.google/technology/ai/google-gemini-ai/#sundar-note', 'title': 'Introducing Gemini: Google’s most capable AI model yet', 'description': 'Gemini is our most capable and general model, built to be multimodal and optimized for three different sizes: Ultra, Pro and Nano.', 'language': 'en-us'})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunked_doc[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0RTQ461PG4X",
        "outputId": "dd3910de-3208-443d-d13d-675862beb81b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Share\\n\\n\\n\\n\\n\\n\\nTwitter\\n\\n\\n\\n\\n\\nFacebook\\n\\n\\n\\n\\n\\nLinkedIn\\n\\n\\n\\n\\n\\nMail\\n\\n\\n\\n\\n\\n\\nCopy link\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n               Latest stories\\n            \\n\\n\\n\\n              Product updates\\n              \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Product updates', metadata={'source': 'https://blog.google/technology/ai/google-gemini-ai/#sundar-note', 'title': 'Introducing Gemini: Google’s most capable AI model yet', 'description': 'Gemini is our most capable and general model, built to be multimodal and optimized for three different sizes: Ultra, Pro and Nano.', 'language': 'en-us'})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunked_doc[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VVdXjGZPG7w",
        "outputId": "f3958e8c-c434-43d0-f1ce-c72bbe8e74aa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Android, Chrome & Play\\n            \\n\\n\\n\\n\\n\\n\\n\\n\\n                  Android\\n                  \\n                \\n\\n\\n\\n                  Chrome', metadata={'source': 'https://blog.google/technology/ai/google-gemini-ai/#sundar-note', 'title': 'Introducing Gemini: Google’s most capable AI model yet', 'description': 'Gemini is our most capable and general model, built to be multimodal and optimized for three different sizes: Ultra, Pro and Nano.', 'language': 'en-us'})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings=HuggingFaceInferenceAPIEmbeddings(\n",
        "    api_key=HF_TOKEN, model_name=\"BAAI/bge-base-en-v1.5\"\n",
        ")"
      ],
      "metadata": {
        "id": "mRe5MY7hPU_0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb= Chroma.from_documents(chunked_doc, embeddings)"
      ],
      "metadata": {
        "id": "O6EMZKmRPVCP"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query=\"what is gemini?\"\n",
        "search=vectordb.similarity_search(query)"
      ],
      "metadata": {
        "id": "oL-yxLNoPVFo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search[0].page_content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ZAYGcqBgP-9w",
        "outputId": "607ac6dd-4326-4cf4-e84a-800ceeebed65"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'10:25\\n\\n\\n\\n\\nGemini unlocks new scientific insights'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriver = vectordb.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={'k':4}\n",
        ")"
      ],
      "metadata": {
        "id": "U9D_L1-MQLMU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriver.get_relevant_documents(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atY-Ik-EQLN_",
        "outputId": "42577cac-2af2-4596-efb3-b58a5121b1b4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='10:25\\n\\n\\n\\n\\nGemini unlocks new scientific insights', metadata={'description': 'Gemini is our most capable and general model, built to be multimodal and optimized for three different sizes: Ultra, Pro and Nano.', 'language': 'en-us', 'source': 'https://blog.google/technology/ai/google-gemini-ai/#sundar-note', 'title': 'Introducing Gemini: Google’s most capable AI model yet'}),\n",
              " Document(page_content='model we’ve ever built.Gemini is the result of large-scale collaborative efforts by teams across Google, including our colleagues at Google Research. It was built from the ground up to be multimodal, which means it can generalize and seamlessly', metadata={'description': 'Gemini is our most capable and general model, built to be multimodal and optimized for three different sizes: Ultra, Pro and Nano.', 'language': 'en-us', 'source': 'https://blog.google/technology/ai/google-gemini-ai/#sundar-note', 'title': 'Introducing Gemini: Google’s most capable AI model yet'}),\n",
              " Document(page_content='Advanced codingOur first version of Gemini can understand, explain and generate high-quality code in the world’s most popular programming languages, like Python, Java, C++, and Go. Its ability to work across languages and reason about complex', metadata={'description': 'Gemini is our most capable and general model, built to be multimodal and optimized for three different sizes: Ultra, Pro and Nano.', 'language': 'en-us', 'source': 'https://blog.google/technology/ai/google-gemini-ai/#sundar-note', 'title': 'Introducing Gemini: Google’s most capable AI model yet'}),\n",
              " Document(page_content='the world. AI that feels less like a smart piece of software and more like something useful and intuitive — an expert helper or assistant.Today, we’re a step closer to this vision as we introduce Gemini, the most capable and general model we’ve ever', metadata={'description': 'Gemini is our most capable and general model, built to be multimodal and optimized for three different sizes: Ultra, Pro and Nano.', 'language': 'en-us', 'source': 'https://blog.google/technology/ai/google-gemini-ai/#sundar-note', 'title': 'Introducing Gemini: Google’s most capable AI model yet'})]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOADING ZEPHYR 7B MODEL AND MAKING PROMPT TO AUGMENT THE MODEL"
      ],
      "metadata": {
        "id": "afqf3dz-Quvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "hf_token= getpass()\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN']=hf_token\n",
        "\n",
        "\n",
        "llm = HuggingFaceHub(\n",
        "    repo_id=\"huggingfaceh4/zephyr-7b-alpha\",\n",
        "    model_kwargs={\"temperature\": 0.5, \"max_length\": 64,\"max_new_tokens\":512}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSasBYslQmFA",
        "outputId": "ca04d01e-e389-44f1-c0a3-320cfff19146"
      },
      "execution_count": 23,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "lHNj5hF6QmIr"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template=\"\"\"\n",
        "<|system|>\n",
        "You are a AI assitant that read the given context and give honest and harmless response.\n",
        "your grammar should be perfect and relevant.\n",
        "\n",
        "</s>\n",
        "<|user|>\n",
        "{query}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "zRfOPP1lSdxl"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=ChatPromptTemplate.from_template(template)\n",
        "output_parser=StrOutputParser()"
      ],
      "metadata": {
        "id": "HsxnCkvhTirt"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain=(\n",
        "     {'context':retriver, 'query':RunnablePassthrough()}\n",
        "    |prompt\n",
        "    |llm\n",
        "    |output_parser\n",
        ")\n",
        ""
      ],
      "metadata": {
        "id": "xUwDuo_YTD-Q"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=rag_chain.invoke(\"what is gemini?\")"
      ],
      "metadata": {
        "id": "L3a8bowZTEB5"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmCHU4NxSd1H",
        "outputId": "c21990fc-7271-416d-ca52-02b44b8c31a9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: \n",
            "<|system|>\n",
            "You are a AI assitant that read the given context and give honest and harmless response.\n",
            "your grammar should be perfect and relevant.\n",
            "\n",
            "</s>\n",
            "<|user|>\n",
            "what is gemini?\n",
            "</s>\n",
            "<|assistant|>\n",
            "Gemini is one of the 12 zodiac signs in astrology, represented by the symbol of the twins. It is the third sign of the zodiac and is associated with the period between May 21 and June 20. People born under this sign are said to be communicative, adaptable, and curious.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response=rag_chain.invoke(\"what is State-of-the-art performance?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mLKoi5FWAib",
        "outputId": "3ae627d4-55d8-468e-b3b2-1565f677f0a2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: \n",
            "<|system|>\n",
            "You are a AI assitant that read the given context and give honest and harmless response.\n",
            "your grammar should be perfect and relevant.\n",
            "\n",
            "</s>\n",
            "<|user|>\n",
            "what is State-of-the-art performance?\n",
            "</s>\n",
            "<|assistant|>\n",
            "State-of-the-art performance refers to the highest level of performance currently achieved in a particular domain or task. It is the best known or established result in that field, and it serves as a benchmark for other researchers or practitioners to strive for. In other words, it is the current best-known solution or approach for a given problem or task.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u_TqlTHKWAmA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}